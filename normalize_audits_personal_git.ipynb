{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fbf0bd5",
   "metadata": {},
   "source": [
    "# Normalize Audits vs Rejects EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c932bcb",
   "metadata": {},
   "source": [
    "Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy import stats\n",
    "import pyodbc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76832ab1",
   "metadata": {},
   "source": [
    "DB connonction details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe3bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#redacted "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af5c6e",
   "metadata": {},
   "source": [
    "SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4335ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_query = \"\"\"\n",
    "-- redacted \n",
    "\"\"\"\n",
    "\n",
    "Joined_view = \"\"\"\n",
    "-- redacted \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c0ade4",
   "metadata": {},
   "source": [
    "Initalize the Data Frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "with audit_conn:\n",
    "    audit_df = pd.read_sql_query(audit_query, audit_conn)\n",
    "\n",
    "\n",
    "\n",
    "with reject_conn:\n",
    "    receipt_date_df = pd.read_sql_query(Joined_view, reject_conn)\n",
    "    \n",
    "\n",
    "\n",
    "audit_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42347440",
   "metadata": {},
   "outputs": [],
   "source": [
    "receipt_date_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec748535",
   "metadata": {},
   "source": [
    "we need to create a audit flag to do this we need to take the receipt_date_df and add a coluam called audit_last_90 that is a 1 or 0 flag column. that flag populats one if the [Supplier_Number] from receipt_date_df = [Supplier] from the audit df and the [audit conducted dt] from audit_df is with in 90 days of [Earliest_Rcvr_Dt] from receipt_date_df. so what i am looking to do is just see the rows from receipt_date_df that had a audit in the last 90 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cbdb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start funtions \n",
    "\n",
    "def flag_receipt_audit(\n",
    "    receipt_df: pd.DataFrame,\n",
    "    audit_df:   pd.DataFrame,\n",
    "    # column names – change only if your source files use something else\n",
    "    supplier_receipt_col: str = \"supplier_no\",          # column in receipt_df\n",
    "    supplier_audit_col:   str = \"Supplier\",            # column in audit_df\n",
    "    receipt_date_col:     str = \"receipt_date\",        # receipt datetime in receipt_df\n",
    "    audit_date_col:       str = \"audit conducted dt\"   # audit datetime in audit_df\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a flag (`audit_last_90`) to `receipt_df` indicating whether the same\n",
    "    supplier had at least one audit in the 90 days preceding each receipt.\n",
    "    Returns a new DataFrame; the original is not modified.\n",
    "    \"\"\"\n",
    "\n",
    "    #  flatten possible MultiIndex columns\n",
    "    for df in (receipt_df, audit_df):\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = [\"_\".join(map(str, t)).strip(\"_\") for t in df.columns]\n",
    "\n",
    "    # remove exact duplicate rows \n",
    "    receipt_df = receipt_df.drop_duplicates().reset_index(drop=True)\n",
    "    audit_df   = audit_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # keep only needed columns and remember original row order \n",
    "    receipt_df = receipt_df.copy()\n",
    "    receipt_df[\"_orig_idx\"] = receipt_df.index                       # helper column\n",
    "    jd = receipt_df[[supplier_receipt_col, receipt_date_col, \"_orig_idx\"]].copy()\n",
    "    ad = audit_df[[supplier_audit_col, audit_date_col]].copy()\n",
    "\n",
    "    # ensure dates are proper datetime objects\n",
    "    jd[receipt_date_col] = pd.to_datetime(jd[receipt_date_col], errors=\"coerce\")\n",
    "    ad[audit_date_col]   = pd.to_datetime(ad[audit_date_col],   errors=\"coerce\")\n",
    "\n",
    "    #  drop rows where supplier or date could not be parsed \n",
    "    jd = jd.dropna(subset=[supplier_receipt_col, receipt_date_col])\n",
    "    ad = ad.dropna(subset=[supplier_audit_col,   audit_date_col])\n",
    "\n",
    "    # left‑join receipts to all audits for the same supplier \n",
    "    merged = jd.merge(\n",
    "        ad,\n",
    "        left_on=supplier_receipt_col,\n",
    "        right_on=supplier_audit_col,\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_rcpt\", \"_audit\")\n",
    "    )\n",
    "\n",
    "    # compute difference in days and flag audits within 0‑90 days \n",
    "    merged[\"day_diff\"] = (merged[receipt_date_col] - merged[audit_date_col]).dt.days\n",
    "    merged[\"audit_in_window\"] = (\n",
    "        (merged[\"day_diff\"] >= 0) & (merged[\"day_diff\"] <= 90)\n",
    "    ).astype(int)\n",
    "\n",
    "    # collapse back to one row per receipt (max flag per receipt) \n",
    "    flag_per_receipt = (\n",
    "        merged.groupby(\"_orig_idx\")[\"audit_in_window\"]\n",
    "              .max()\n",
    "              .rename(\"audit_last_90\")\n",
    "    )\n",
    "\n",
    "    # attach flag to the original receipt DataFrame \n",
    "    receipt_audit = receipt_df.copy()\n",
    "    receipt_audit[\"audit_last_90\"] = (\n",
    "        flag_per_receipt.reindex(receipt_audit[\"_orig_idx\"])\n",
    "                       .fillna(0)\n",
    "                       .astype(int)\n",
    "    )\n",
    "    receipt_audit = receipt_audit.drop(columns=[\"_orig_idx\"])\n",
    "\n",
    "    return receipt_audit\n",
    "\n",
    "\n",
    "\n",
    "receipt_audit = flag_receipt_audit(\n",
    "    receipt_df=receipt_date_df,\n",
    "    audit_df=audit_df,\n",
    "    supplier_receipt_col=\"supplier_no\",\n",
    "    supplier_audit_col=\"Supplier\",\n",
    "    receipt_date_col=\"receipt_date\",\n",
    "    audit_date_col=\"audit conducted dt\"\n",
    ")\n",
    "\n",
    "# summary tables to chech distribution \n",
    "print(\"\\n=== audit_last_90 flag distribution ===\")\n",
    "print(receipt_audit[\"audit_last_90\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea64788",
   "metadata": {},
   "outputs": [],
   "source": [
    "receipt_audit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea230db9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9620d943",
   "metadata": {},
   "source": [
    "summary tables- now we need to combine all the flags we have set up. One thing to remeber is that when the flag for reject is found that means that no reject occored. Keep that in mind as we provide logic for the tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f574d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#create the 4 tables using logical tests \n",
    "df = receipt_audit.copy()\n",
    "\n",
    "df[\"audit_non_rejects\"]         = (df[\"audit_last_90\"] == 1) & (df[\"reject_flag\"] == 1)\n",
    "df[\"audit_rejects\"]     = (df[\"audit_last_90\"] == 1) & (df[\"reject_flag\"] == 0)\n",
    "df[\"non_audit_non_rejects\"]     = (df[\"audit_last_90\"] == 0) & (df[\"reject_flag\"] == 1)\n",
    "df[\"non_audit_rejects\"] = (df[\"audit_last_90\"] == 0) & (df[\"reject_flag\"] == 0)\n",
    "\n",
    "# Convert the booleans to integers so they can be summed even though they shouls allready be integers\n",
    "bool_cols = [\n",
    "    \"audit_rejects\",\n",
    "    \"audit_non_rejects\",\n",
    "    \"non_audit_rejects\",\n",
    "    \"non_audit_non_rejects\"\n",
    "]\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "# group by supplier and count flags \n",
    "Summary_counts = (\n",
    "    df\n",
    "    .groupby(\"supplier_no\")[bool_cols]   # keep only the four columns we care about\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "#clean up names just to make sure i did not add spaces where i should not have\n",
    "Summary_counts = Summary_counts.rename_axis(None, axis=1)\n",
    "\n",
    "\n",
    "Summary_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b94552",
   "metadata": {},
   "source": [
    "We only want to show rows where each of the counts is greater then 5. I did this to ensure that suppliers with low ammounts of receipts are not taken into account as small changes can have bigger impacts of the ratios later down the road. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8142af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to only show rows that dont have 0s in the count fields \n",
    "count_cols = [\n",
    "    \"audit_rejects\",\n",
    "    \"audit_non_rejects\",\n",
    "    \"non_audit_rejects\",\n",
    "    \"non_audit_non_rejects\"\n",
    "]\n",
    "\n",
    "filtered_SC = (\n",
    "    Summary_counts[ (Summary_counts[count_cols] > 0).all(axis=1) ]\n",
    "                .reset_index(drop=True)      # clean index\n",
    ")\n",
    "\n",
    "\n",
    "filtered_SC.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01884982",
   "metadata": {},
   "source": [
    "Here we create the ratios of rejects for both audited and non audited reciepts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e6752",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  Build the ratio DataFrame\n",
    "\n",
    "filtered_ratio_SC = (\n",
    "    filtered_SC[['supplier_no']]                             \n",
    "    .assign(\n",
    "        audit_ratio = filtered_SC['audit_non_rejects'] / filtered_SC['audit_rejects'],\n",
    "        non_audit_ratio = filtered_SC['non_audit_non_rejects'] / filtered_SC['non_audit_rejects']\n",
    "    )\n",
    ")\n",
    "\n",
    "# clean up any infinite or NaN values that arise from a 0 divisor\n",
    "\n",
    "filtered_ratio_SC['audit_ratio'] = filtered_ratio_SC['audit_ratio'].replace([float('inf'), -float('inf')], pd.NA)\n",
    "filtered_ratio_SC['non_audit_ratio'] = filtered_ratio_SC['non_audit_ratio'].replace([float('inf'), -float('inf')], pd.NA)\n",
    "\n",
    "filtered_ratio_SC.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04941a4",
   "metadata": {},
   "source": [
    "Here we run the t test to see the avrage diffrence between the 2 values, the p test to tell if the diffrance is statisticaly significant and the Cohen's d (paired) test to determine the level of magnitude in diffracne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d22a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audit_vals     = filtered_ratio_SC['audit_ratio'].values\n",
    "non_audit_vals = filtered_ratio_SC['non_audit_ratio'].values\n",
    "\n",
    "# paired (“related samples”) t‑test\n",
    "t_stat, p_val = stats.ttest_rel(non_audit_vals, audit_vals, nan_policy='omit')\n",
    "\n",
    "print('--- Paired t‑test results ---')\n",
    "print(f't‑statistic : {t_stat:.4f}')\n",
    "print(f'p‑value    : {p_val:.4g}')\n",
    "\n",
    "\n",
    "# effect size (Cohen’s d for paired data)\n",
    "\n",
    "diff = non_audit_vals - audit_vals\n",
    "cohen_d = diff.mean() / diff.std(ddof=1)  \n",
    "print(f\"Cohen's d (paired) : {cohen_d:.3f}\")\n",
    "\n",
    "# lets look for outliers with a quick diffrance chart\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(diff, kde=True, bins=15, color='steelblue')\n",
    "plt.title('Distribution of (non_audit_ratio – audit_ratio)')\n",
    "plt.xlabel('Difference')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614e55ff",
   "metadata": {},
   "source": [
    "We saw a few very extreme values in the difference between non‑audit and audit ratios that were making a tiny but unrealistic statistical significance look like audits caused more rejects. To fix this we applied the standard box‑plot rule: we calculated the inter‑quartile range, set the lower and upper limits at Q1 minus 1.5 times the IQR and Q3 plus 1.5 times the IQR, and dropped any points outside those limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Compute the difference\n",
    "diff = filtered_ratio_SC['non_audit_ratio'] - filtered_ratio_SC['audit_ratio']\n",
    "\n",
    "# 2. Find IQR and whisker fences\n",
    "q1 = diff.quantile(0.25)\n",
    "q3 = diff.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_fence = q1 - 1.5 * iqr\n",
    "upper_fence = q3 + 1.5 * iqr\n",
    "\n",
    "# 3. Keep only non‑outlier rows (mask stays aligned for both ratios)\n",
    "mask = (diff >= lower_fence) & (diff <= upper_fence)\n",
    "audit_clean     = filtered_ratio_SC.loc[mask, 'audit_ratio'].values\n",
    "non_audit_clean = filtered_ratio_SC.loc[mask, 'non_audit_ratio'].values\n",
    "diff_clean      = diff[mask].values\n",
    "\n",
    "# 4. Paired t‑test & Cohen's d on the cleaned data\n",
    "t_stat, p_val = stats.ttest_rel(non_audit_clean, audit_clean, nan_policy='omit')\n",
    "cohen_d = diff_clean.mean() / diff_clean.std(ddof=1)\n",
    "\n",
    "print('t‑statistic :', round(t_stat, 4))\n",
    "print('p‑value    :', round(p_val, 4))\n",
    "print(\"Cohen's d  :\", round(cohen_d, 3))\n",
    "\n",
    "# 5. Visualise the cleaned distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(diff_clean, kde=True, bins=15, color='steelblue')\n",
    "plt.title('Difference after outlier removal')\n",
    "plt.xlabel('non_audit_ratio - audit_ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec6d8c2",
   "metadata": {},
   "source": [
    "From this analysis we see that the difference in ratios is not significantly different, and therefore there is no reason to think that audits have an effect on rejects once we normalize to receipt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My New Kernel)",
   "language": "python",
   "name": "my_new_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0rc3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
